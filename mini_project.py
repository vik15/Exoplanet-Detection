# -*- coding: utf-8 -*-
"""Mini_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lbz7gbzh4TSpW8wnr_3tFhICbfzCYWGL

Lets us start by importing all the libraries:
"""

import os
import warnings
import math
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
from pylab import rcParams
rcParams['figure.figsize'] = 10, 6
from sklearn.metrics import mean_squared_error, mean_absolute_error
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split 
from sklearn import linear_model
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_score, recall_score,roc_curve,auc, f1_score, roc_auc_score,confusion_matrix, accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler, normalize
from scipy import ndimage
import seaborn as sns
import pickle

"""Mounting Google Drive"""

# from google.colab import drive
# drive.mount('/content/drive')

"""Load the train and test data.


"""

test_data = pd.read_csv('csv/exoTest.csv').fillna(0)

train_data = pd.read_csv('csv/exoTrain.csv').fillna(0)

# train_data.head()

"""Now the target column LABEL which consists of two categories 1(Does not represents exoplanet) and 2(represents the presence of exoplanet). So, we convert them to binary values (0, 1) for easier processing of data."""

train_data['LABEL'].replace({1: 0, 2: 1},inplace = True)
test_data['LABEL'].replace({1: 0, 2: 1},inplace = True)

# train_data.columns

# train_data.shape

# train_data.describe()

# train_data.info()

"""Checking the Count of Values in the LABEL Column"""

# train_data.LABEL.value_counts()

"""Checking Null Value in the Dataset"""

# sns.heatmap(train_data.isnull());

# """Correlation in the Data"""

# plt.figure(figsize=(15,15))
# sns.heatmap(train_data.corr())
# plt.title('Correlation in the data')
# plt.show()

# test_data.shape

"""Now we visualize the target column in the train_dataset and get an idea about the class distribution."""

# plt.figure(figsize=(4,8))
# colors = ["0", "1"]
# sns.countplot('LABEL', data=train_data, palette = "Set2")
# plt.title('Class Distributions \n (0: Not Exoplanet || 1: Exoplanet)', fontsize=14)

"""Let us plot the first 4 rows of the train data and observe the intensity of flux values."""

# from pylab import rcParams
# rcParams['figure.figsize'] = 13, 8
# plt.title('Distribution of flux values', fontsize=10)
# plt.xlabel('Flux values')
# plt.ylabel('Flux intensity')
# plt.plot(train_data.iloc[0,],c = 'b')
# plt.plot(train_data.iloc[1,],c = 'springgreen')
# plt.plot(train_data.iloc[2,],c = 'orange')
# plt.plot(train_data.iloc[3,],c = 'r')
# plt.show()

"""Let us plot the Gaussian histogram of non-exoplanets data."""

# labels_1=[100,200,300]
# for i in labels_1:
#     plt.hist(train_data.iloc[i,:], bins=200)
#     plt.title("Gaussian Histogram")
#     plt.xlabel("Flux values")
#     plt.show()

"""Now plot Gaussian histogram of the data when exoplanets are present."""

# labels_1=[16,21,25]
# for i in labels_1:
#     plt.hist(train_data.iloc[i,:], bins=200)
#     plt.title("Gaussian Histogram")
#     plt.xlabel("Flux values")
#     plt.show()

"""So let us first split our dataset and normalize it.


"""

x_train = train_data.drop(["LABEL"],axis=1)
y_train = train_data["LABEL"]   
x_test = test_data.drop(["LABEL"],axis=1)
y_test = test_data["LABEL"]

"""Well, our data is clean but is not normalized. """

x_train = normalized = normalize(x_train)
x_test = normalize(x_test)

"""The next step is to apply gaussian filters to both test and train.

"""

x_train = filtered = ndimage.filters.gaussian_filter(x_train, sigma=10)
x_test = ndimage.filters.gaussian_filter(x_test, sigma=10)

"""We use feature scaling so that all the values remain in the comparable range.


"""

#Feature scaling
std_scaler = StandardScaler()
x_train = scaled = std_scaler.fit_transform(x_train)
x_test = std_scaler.fit_transform(x_test)

"""The number of columns/features that we have been working with is huge. We have 5087 rows and 3198 columns in our training dataset. Basically we need to decrease the number of features(Dimensionality Reduction) to remove the possibility of Curse of Dimensionality.

To perform Principal Component Analysis (PCA) we have to choose the number of features/dimensions that we want in our data.
"""

#Dimensionality reduction
from sklearn.decomposition import PCA
pca = PCA() 
x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)
total=sum(pca.explained_variance_)
k=0
current_variance=0
while current_variance/total < 0.90:
    current_variance += pca.explained_variance_[k]
    k=k+1
k

"""Now we have k = 37 and apply PCA on our independent variables.


"""

#Apply PCA with n_componenets
pca = PCA(n_components=k)
x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)
plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)') #for each component
plt.title('Exoplanet Dataset Explained Variance')
plt.show()

"""Here we use the SMOTE(Synthetic Minority Over-sampling TEchnique) resampling method."""

print("Before OverSampling, counts of label '1': {}".format(sum(y_train==1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train==0)))
sm = SMOTE(random_state=27, sampling_strategy = 1.0)
x_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())
print("After OverSampling, counts of label '1': {}".format(sum(y_train_res==1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res==0)))

"""Now it comes to building a model which can classify exoplanets on the test data.

So Iâ€™ll create a function model which will:

1.   Fit the model

2.   Perform Cross-validation

3.   Check the Accuracy of our model

4.   Generate Classification report

5.   Generate Confusion matrix

"""

def model(classifier,dtrain_x,dtrain_y,dtest_x,dtest_y):
    #fit the model
    classifier.fit(dtrain_x,dtrain_y)
    predictions = classifier.predict(dtest_x)

    #Cross validation
    accuracies = cross_val_score(estimator = classifier, X = x_train_res, y = y_train_res, cv = 5, n_jobs = -1)
    mean = accuracies.mean()
    variance = accuracies.std()
    print("Accuracy mean: "+ str(mean))
    print("Accuracy variance: "+ str(variance))

    #Accuracy
    print ("\naccuracy_score :",accuracy_score(dtest_y,predictions))

    #Classification report
    print ("\nclassification report :\n",(classification_report(dtest_y,predictions)))

    #Confusion matrix
    plt.figure(figsize=(13,10))
    plt.subplot(221)
    sns.heatmap(confusion_matrix(dtest_y,predictions),annot=True,cmap="viridis",fmt = "d",linecolor="k",linewidths=3)
    plt.title("CONFUSION MATRIX",fontsize=20)

"""Now we fit the Support Vector Machine (SVM) algorithm to the training set.


"""

from sklearn.svm import SVC
SVM_model=SVC()
model(SVM_model,x_train_res,y_train_res,x_test,y_test)

"""Here we are calling the Random forest classification algorithm."""

# and call the Random forest classification algorithm.
from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier()

model(rf_classifier,x_train_res,y_train_res,x_test,y_test)


pickle.dump(rf_classifier, open('model.pkl','wb'))

model = pickle.load(open('model.pkl','rb'))
print(model.predict([[ 17.77629469, -35.98307765,  22.54208581, -10.06983073,
       -21.5403753 ,  -3.61090104, -26.2241671 , -17.07502809,
       -15.34758935,  14.18821438, -21.12685013,  14.92675097,
         2.2972566 ,  16.30920354,   6.2872877 ,   0.41244117,
        -8.01768274,  -2.92854169,  -1.79971481, -16.31950274,
        -2.34664529,   2.3541612 ,  10.47336413,   5.51508743,
        -4.59963926,  -3.32066871,   6.89355126,   4.82884901,
         0.7424178 ,   0.20100378,   2.9811181 ,  -0.35507926,
         1.77676107,  -7.45315299,  11.28674306,  -6.99151242,
         7.11982244]]))